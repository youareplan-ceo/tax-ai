#!/usr/bin/env python3
"""
YouArePlan EasyTax v8 - LLM ê³ ë„í™” ìŠ¤í¬ë¦½íŠ¸
OpenAI API í‚¤ ê²€ì¦, ë¶„ë¥˜ ì •í™•ë„ í…ŒìŠ¤íŠ¸, ë¹„ìš© ëª¨ë‹ˆí„°ë§ í†µí•©
"""

import os, json, time, asyncio
import requests
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass
import statistics

@dataclass
class TestCase:
    """AI ë¶„ë¥˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    vendor: str
    amount: float
    memo: str
    expected_account: str
    expected_tax: str
    description: str

@dataclass
class ClassificationResult:
    """ë¶„ë¥˜ ê²°ê³¼"""
    account_code: str
    tax_type: str
    confidence: float
    reasoning: str
    response_time_ms: float
    cost_usd: float
    tokens_used: int

class LLMEnhancer:
    def __init__(self, base_url: str = "http://localhost:8081"):
        self.base_url = base_url
        self.results = []
        self.total_cost = 0.0
        
        # í•œêµ­ ì„¸ë¬´ ê¸°ì¤€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        self.test_cases = [
            TestCase("ìŠ¤íƒ€ë²…ìŠ¤ ì½”ë¦¬ì•„", 15000, "íŒ€ íšŒì˜ìš© ì»¤í”¼", "ë³µë¦¬í›„ìƒë¹„", "ë¶ˆê³µì œ", "ì»¤í”¼ - ë³µë¦¬í›„ìƒë¹„/ë¶ˆê³µì œ"),
            TestCase("ë„¤ì´ë²„ í´ë¼ìš°ë“œ", 45000, "ì„œë²„ í˜¸ìŠ¤íŒ… ë¹„ìš©", "í†µì‹ ë¹„", "ê³¼ì„¸", "í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ - í†µì‹ ë¹„/ê³¼ì„¸"),
            TestCase("ì˜¤í”¼ìŠ¤ë””í¬", 8500, "A4 ìš©ì§€ êµ¬ë§¤", "ì†Œëª¨í’ˆë¹„", "ê³¼ì„¸", "ì‚¬ë¬´ìš©í’ˆ - ì†Œëª¨í’ˆë¹„/ê³¼ì„¸"),
            TestCase("í˜„ëŒ€ì¹´ë“œ", 150000, "ê³ ê° ì ‘ëŒ€ ì‹ì‚¬", "ì ‘ëŒ€ë¹„", "ë¶ˆê³µì œ", "ì ‘ëŒ€ë¹„ - ë¶ˆê³µì œ"),
            TestCase("ë¡¯ë°ë§ˆíŠ¸", 25000, "ì‚¬ë¬´ì‹¤ ì²­ì†Œìš©í’ˆ", "ì†Œëª¨í’ˆë¹„", "ê³¼ì„¸", "ì²­ì†Œìš©í’ˆ - ì†Œëª¨í’ˆë¹„/ê³¼ì„¸"),
            TestCase("KT", 89000, "ì‚¬ë¬´ì‹¤ ì¸í„°ë„·", "í†µì‹ ë¹„", "ê³¼ì„¸", "í†µì‹  ì„œë¹„ìŠ¤ - í†µì‹ ë¹„/ê³¼ì„¸"),
            TestCase("êµë³´ë¬¸ê³ ", 35000, "ì—…ë¬´ ê´€ë ¨ ë„ì„œ", "ë„ì„œì¸ì‡„ë¹„", "ê³¼ì„¸", "ë„ì„œ êµ¬ë§¤ - ë„ì„œì¸ì‡„ë¹„/ê³¼ì„¸"),
            TestCase("ì‹ í•œì€í–‰", 3000, "ê³„ì¢Œì´ì²´ ìˆ˜ìˆ˜ë£Œ", "ì§€ê¸‰ìˆ˜ìˆ˜ë£Œ", "ê³¼ì„¸", "ì€í–‰ ìˆ˜ìˆ˜ë£Œ - ì§€ê¸‰ìˆ˜ìˆ˜ë£Œ/ê³¼ì„¸"),
            TestCase("LGì „ì", 1200000, "ì‚¬ë¬´ì‹¤ ì—ì–´ì»¨", "ë¹„í’ˆ", "ê³¼ì„¸", "ì‚¬ë¬´ìš© ë¹„í’ˆ - ë¹„í’ˆ/ê³¼ì„¸"),
            TestCase("êµ­ì„¸ì²­", 50000, "ë¶€ê°€ì„¸ ë‚©ë¶€", "ì¡°ì„¸ê³µê³¼", "ë¶ˆê³µì œ", "ì„¸ê¸ˆ ë‚©ë¶€ - ì¡°ì„¸ê³µê³¼/ë¶ˆê³µì œ")
        ]
    
    def check_api_status(self) -> Dict[str, Any]:
        """API ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/api/status", timeout=10)
            return response.json()
        except Exception as e:
            return {"error": str(e), "available": False}
    
    def classify_single_entry(self, test_case: TestCase) -> ClassificationResult:
        """ë‹¨ì¼ ê±°ë˜ë‚´ì—­ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸"""
        payload = {
            "text_context": f"ê±°ë˜ì²˜: {test_case.vendor}, ê¸ˆì•¡: {test_case.amount:,.0f}ì›, ë©”ëª¨: {test_case.memo}",
            "use_llm": True,
            "need_reasoning": True
        }
        
        start_time = time.time()
        try:
            response = requests.post(f"{self.base_url}/ai/classify-entry", json=payload, timeout=30)
            response_time_ms = (time.time() - start_time) * 1000
            
            if response.status_code == 200:
                data = response.json()
                result_data = data.get("data", {})
                tokens = data.get("tokens", {})
                
                # ë¹„ìš© ì¶”ì • (GPT-4o-mini ê¸°ì¤€)
                cost_per_1k_input = 0.00015  # $0.15/1M tokens
                cost_per_1k_output = 0.0006  # $0.60/1M tokens
                input_tokens = tokens.get("input", 150)
                output_tokens = tokens.get("output", 50)
                cost_usd = (input_tokens * cost_per_1k_input / 1000) + (output_tokens * cost_per_1k_output / 1000)
                
                return ClassificationResult(
                    account_code=result_data.get("account_code", "ë¯¸ë¶„ë¥˜"),
                    tax_type=result_data.get("tax_type", "ê³¼ì„¸"),
                    confidence=result_data.get("confidence", 0.0),
                    reasoning=result_data.get("reason", ""),
                    response_time_ms=response_time_ms,
                    cost_usd=cost_usd,
                    tokens_used=input_tokens + output_tokens
                )
            else:
                return ClassificationResult("ì˜¤ë¥˜", "ê³¼ì„¸", 0.0, f"HTTP {response.status_code}", response_time_ms, 0.0, 0)
                
        except Exception as e:
            response_time_ms = (time.time() - start_time) * 1000
            return ClassificationResult("ì˜¤ë¥˜", "ê³¼ì„¸", 0.0, str(e), response_time_ms, 0.0, 0)
    
    def run_accuracy_test(self) -> Dict[str, Any]:
        """ë¶„ë¥˜ ì •í™•ë„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª AI ë¶„ë¥˜ ì •í™•ë„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        results = []
        total_cost = 0.0
        response_times = []
        
        for i, test_case in enumerate(self.test_cases, 1):
            print(f"  {i}/{len(self.test_cases)}: {test_case.description}")
            
            result = self.classify_single_entry(test_case)
            results.append({
                "test_case": test_case.description,
                "input": {
                    "vendor": test_case.vendor,
                    "amount": test_case.amount,
                    "memo": test_case.memo
                },
                "expected": {
                    "account_code": test_case.expected_account,
                    "tax_type": test_case.expected_tax
                },
                "actual": {
                    "account_code": result.account_code,
                    "tax_type": result.tax_type,
                    "confidence": result.confidence,
                    "reasoning": result.reasoning
                },
                "performance": {
                    "response_time_ms": result.response_time_ms,
                    "cost_usd": result.cost_usd,
                    "tokens_used": result.tokens_used
                },
                "accuracy": {
                    "account_match": result.account_code == test_case.expected_account,
                    "tax_match": result.tax_type == test_case.expected_tax,
                    "both_match": (result.account_code == test_case.expected_account) and (result.tax_type == test_case.expected_tax)
                }
            })
            
            total_cost += result.cost_usd
            response_times.append(result.response_time_ms)
            
            # API ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            time.sleep(0.5)
        
        # í†µê³„ ê³„ì‚°
        account_accuracy = sum(1 for r in results if r["accuracy"]["account_match"]) / len(results)
        tax_accuracy = sum(1 for r in results if r["accuracy"]["tax_match"]) / len(results)
        overall_accuracy = sum(1 for r in results if r["accuracy"]["both_match"]) / len(results)
        
        avg_response_time = statistics.mean(response_times)
        median_response_time = statistics.median(response_times)
        
        summary = {
            "test_summary": {
                "total_tests": len(results),
                "account_accuracy": round(account_accuracy * 100, 1),
                "tax_accuracy": round(tax_accuracy * 100, 1),
                "overall_accuracy": round(overall_accuracy * 100, 1),
                "avg_response_time_ms": round(avg_response_time, 2),
                "median_response_time_ms": round(median_response_time, 2),
                "total_cost_usd": round(total_cost, 6),
                "estimated_monthly_cost_usd": round(total_cost * 30 * 100, 2)  # ì›” 3000ê±´ ê¸°ì¤€
            },
            "detailed_results": results,
            "test_timestamp": datetime.now().isoformat()
        }
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì „ì²´ ì •í™•ë„: {overall_accuracy*100:.1f}%")
        return summary
    
    def run_performance_benchmark(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸
        concurrent_requests = [5, 10, 20]
        benchmark_results = {}
        
        for concurrent in concurrent_requests:
            print(f"  ë™ì‹œ ìš”ì²­ {concurrent}ê°œ í…ŒìŠ¤íŠ¸...")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ë™ì‹œ ìš”ì²­
            test_case = self.test_cases[0]
            payload = {
                "text_context": f"ê±°ë˜ì²˜: {test_case.vendor}, ê¸ˆì•¡: {test_case.amount}ì›, ë©”ëª¨: {test_case.memo}",
                "use_llm": True,
                "need_reasoning": False  # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” reasoning ìƒëµ
            }
            
            start_time = time.time()
            success_count = 0
            response_times = []
            
            # ë™ê¸°ì ìœ¼ë¡œ ìˆœì°¨ ìš”ì²­ (ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸)
            for _ in range(concurrent):
                req_start = time.time()
                try:
                    response = requests.post(f"{self.base_url}/ai/classify-entry", json=payload, timeout=10)
                    req_time = (time.time() - req_start) * 1000
                    if response.status_code == 200:
                        success_count += 1
                        response_times.append(req_time)
                except Exception:
                    pass
                time.sleep(0.1)  # ìš”ì²­ ê°„ê²©
            
            total_time = (time.time() - start_time) * 1000
            
            benchmark_results[f"concurrent_{concurrent}"] = {
                "requests": concurrent,
                "successful": success_count,
                "success_rate": round((success_count / concurrent) * 100, 1),
                "total_time_ms": round(total_time, 2),
                "avg_response_time_ms": round(statistics.mean(response_times) if response_times else 0, 2),
                "requests_per_second": round(success_count / (total_time / 1000), 2) if total_time > 0 else 0
            }
        
        print("âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
        return benchmark_results
    
    def run_cost_monitoring(self) -> Dict[str, Any]:
        """ë¹„ìš© ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜"""
        print("ğŸ’° ë¹„ìš© ëª¨ë‹ˆí„°ë§ ì‹œë®¬ë ˆì´ì…˜...")
        
        # ë‹¤ì–‘í•œ ì‚¬ìš©ëŸ‰ ì‹œë‚˜ë¦¬ì˜¤
        scenarios = [
            {"name": "ì†Œê·œëª¨ (ì›” 100ê±´)", "requests_per_month": 100},
            {"name": "ì¤‘ê°„ ê·œëª¨ (ì›” 1,000ê±´)", "requests_per_month": 1000},
            {"name": "ëŒ€ê·œëª¨ (ì›” 10,000ê±´)", "requests_per_month": 10000},
            {"name": "ì—”í„°í”„ë¼ì´ì¦ˆ (ì›” 100,000ê±´)", "requests_per_month": 100000}
        ]
        
        # í‰ê·  í† í° ì‚¬ìš©ëŸ‰ (ì‹¤ì œ API ì‘ë‹µ ê¸°ë°˜)
        avg_input_tokens = 180
        avg_output_tokens = 60
        cost_per_1k_input = 0.00015  # GPT-4o-mini
        cost_per_1k_output = 0.0006
        
        cost_projections = []
        
        for scenario in scenarios:
            monthly_requests = scenario["requests_per_month"]
            
            # ì›”ê°„ ë¹„ìš© ê³„ì‚°
            monthly_input_cost = (monthly_requests * avg_input_tokens * cost_per_1k_input) / 1000
            monthly_output_cost = (monthly_requests * avg_output_tokens * cost_per_1k_output) / 1000
            monthly_total = monthly_input_cost + monthly_output_cost
            
            cost_projections.append({
                "scenario": scenario["name"],
                "monthly_requests": monthly_requests,
                "monthly_cost_usd": round(monthly_total, 2),
                "yearly_cost_usd": round(monthly_total * 12, 2),
                "cost_per_request_usd": round(monthly_total / monthly_requests, 6),
                "tokens_per_month": monthly_requests * (avg_input_tokens + avg_output_tokens)
            })
        
        monitoring_config = {
            "cost_alerts": {
                "daily_limit_usd": 10.0,
                "monthly_limit_usd": 200.0,
                "alert_threshold_percent": 80
            },
            "optimization_tips": [
                "use_llm=Falseë¡œ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ìš°ì„  ì‚¬ìš©",
                "need_reasoning=Falseë¡œ ê°„ë‹¨í•œ ë¶„ë¥˜ì—ì„œ í† í° ì ˆì•½",
                "ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ íšŸìˆ˜ ìµœì í™”",
                "ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ë¶„ë¥˜ ìš”ì²­ ë°©ì§€"
            ]
        }
        
        print("âœ… ë¹„ìš© ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ")
        return {
            "cost_projections": cost_projections,
            "monitoring_config": monitoring_config,
            "model_pricing": {
                "gpt_4o_mini": {
                    "input_per_1m_tokens": 0.15,
                    "output_per_1m_tokens": 0.60
                }
            }
        }
    
    def generate_llm_report(self) -> Dict[str, Any]:
        """LLM ê³ ë„í™” ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ“Š LLM ê³ ë„í™” ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # API ìƒíƒœ í™•ì¸
        api_status = self.check_api_status()
        
        # ì •í™•ë„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        accuracy_results = self.run_accuracy_test()
        
        # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        performance_results = self.run_performance_benchmark()
        
        # ë¹„ìš© ëª¨ë‹ˆí„°ë§ ì„¤ì •
        cost_monitoring = self.run_cost_monitoring()
        
        # ì¢…í•© ë¦¬í¬íŠ¸
        report = {
            "llm_enhancement_report": {
                "timestamp": datetime.now().isoformat(),
                "version": "v8",
                "api_status": api_status,
                "accuracy_analysis": accuracy_results,
                "performance_benchmark": performance_results,
                "cost_monitoring": cost_monitoring,
                "recommendations": [
                    "í˜„ì¬ API í‚¤ê°€ ë°ëª¨ ëª¨ë“œë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ - ì‹¤ì œ ìš´ì˜ ì‹œ ì •í’ˆ í‚¤ í•„ìš”",
                    f"ë¶„ë¥˜ ì •í™•ë„ {accuracy_results['test_summary']['overall_accuracy']}% - ì¶”ê°€ í•™ìŠµ ë°ì´í„°ë¡œ ê°œì„  ê°€ëŠ¥",
                    f"í‰ê·  ì‘ë‹µì‹œê°„ {accuracy_results['test_summary']['avg_response_time_ms']}ms - ì‹¤ì‹œê°„ ë¶„ë¥˜ì— ì í•©",
                    "ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•´ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ì™€ AI ë¶„ë¥˜ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ê¶Œì¥"
                ],
                "next_steps": [
                    "ì‹¤ì œ OpenAI API í‚¤ ì„¤ì •",
                    "í”„ë¡œë•ì…˜ í™˜ê²½ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ êµ¬ì¶•",
                    "ë¹„ìš© ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„",
                    "ë¶„ë¥˜ ì •í™•ë„ ê°œì„ ì„ ìœ„í•œ íŒŒì¸íŠœë‹ ê²€í† "
                ]
            }
        }
        
        return report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ YouArePlan EasyTax v8 - LLM ê³ ë„í™” ì‹œì‘")
    print("=" * 60)
    
    enhancer = LLMEnhancer()
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    report = enhancer.generate_llm_report()
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
    reports_dir = "reports"
    os.makedirs(reports_dir, exist_ok=True)
    
    # JSON ë¦¬í¬íŠ¸
    json_path = f"{reports_dir}/llm_enhancement_report.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # HTML ë¦¬í¬íŠ¸ ìƒì„±
    html_content = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YouArePlan EasyTax v8 - LLM ê³ ë„í™” ë¦¬í¬íŠ¸</title>
    <style>
        body {{ font-family: 'Noto Sans KR', sans-serif; margin: 0; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; border-radius: 16px; box-shadow: 0 8px 30px rgba(0,0,0,0.2); overflow: hidden; }}
        .header {{ background: linear-gradient(135deg, #667eea, #764ba2); color: white; padding: 2rem; text-align: center; }}
        .section {{ padding: 2rem; border-bottom: 1px solid #eee; }}
        .accuracy-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; }}
        .stat-card {{ background: #f8f9fa; padding: 1rem; border-radius: 8px; text-align: center; }}
        .stat-number {{ font-size: 2rem; font-weight: bold; color: #667eea; }}
        .recommendations {{ background: #e3f2fd; padding: 1.5rem; border-radius: 8px; margin: 1rem 0; }}
        .performance-table {{ width: 100%; border-collapse: collapse; margin: 1rem 0; }}
        .performance-table th, .performance-table td {{ padding: 0.75rem; border: 1px solid #ddd; text-align: left; }}
        .performance-table th {{ background: #f8f9fa; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ§  LLM ê³ ë„í™” ë¦¬í¬íŠ¸</h1>
            <p>YouArePlan EasyTax v8 - AI ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™”</p>
            <p>{report['llm_enhancement_report']['timestamp']}</p>
        </div>
        
        <div class="section">
            <h2>ğŸ“Š ë¶„ë¥˜ ì •í™•ë„ ë¶„ì„</h2>
            <div class="accuracy-grid">
                <div class="stat-card">
                    <div class="stat-number">{report['llm_enhancement_report']['accuracy_analysis']['test_summary']['overall_accuracy']}%</div>
                    <div>ì „ì²´ ì •í™•ë„</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">{report['llm_enhancement_report']['accuracy_analysis']['test_summary']['account_accuracy']}%</div>
                    <div>ê³„ì •ê³¼ëª© ì •í™•ë„</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">{report['llm_enhancement_report']['accuracy_analysis']['test_summary']['tax_accuracy']}%</div>
                    <div>ì„¸ê¸ˆìœ í˜• ì •í™•ë„</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">{report['llm_enhancement_report']['accuracy_analysis']['test_summary']['avg_response_time_ms']}ms</div>
                    <div>í‰ê·  ì‘ë‹µì‹œê°„</div>
                </div>
            </div>
        </div>
        
        <div class="section">
            <h2>ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬</h2>
            <table class="performance-table">
                <thead>
                    <tr><th>ë™ì‹œ ìš”ì²­ ìˆ˜</th><th>ì„±ê³µë¥ </th><th>í‰ê·  ì‘ë‹µì‹œê°„</th><th>ì´ˆë‹¹ ì²˜ë¦¬ ìš”ì²­</th></tr>
                </thead>
                <tbody>
    """
    
    for key, perf in report['llm_enhancement_report']['performance_benchmark'].items():
        html_content += f"""
                    <tr>
                        <td>{perf['requests']}ê°œ</td>
                        <td>{perf['success_rate']}%</td>
                        <td>{perf['avg_response_time_ms']}ms</td>
                        <td>{perf['requests_per_second']} req/s</td>
                    </tr>"""
    
    html_content += f"""
                </tbody>
            </table>
        </div>
        
        <div class="section">
            <h2>ğŸ’° ë¹„ìš© ë¶„ì„</h2>
            <table class="performance-table">
                <thead>
                    <tr><th>ì‚¬ìš© ê·œëª¨</th><th>ì›”ê°„ ìš”ì²­</th><th>ì›”ê°„ ë¹„ìš©</th><th>ìš”ì²­ë‹¹ ë¹„ìš©</th></tr>
                </thead>
                <tbody>
    """
    
    for proj in report['llm_enhancement_report']['cost_monitoring']['cost_projections']:
        html_content += f"""
                    <tr>
                        <td>{proj['scenario']}</td>
                        <td>{proj['monthly_requests']:,}ê±´</td>
                        <td>${proj['monthly_cost_usd']}</td>
                        <td>${proj['cost_per_request_usd']}</td>
                    </tr>"""
    
    html_content += f"""
                </tbody>
            </table>
        </div>
        
        <div class="section">
            <h2>ğŸ’¡ ê¶Œì¥ì‚¬í•­</h2>
            <div class="recommendations">
                <ul>
    """
    
    for rec in report['llm_enhancement_report']['recommendations']:
        html_content += f"<li>{rec}</li>"
    
    html_content += """
                </ul>
            </div>
        </div>
        
        <div class="section" style="border-bottom: none;">
            <h2>ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„</h2>
            <ol>
    """
    
    for step in report['llm_enhancement_report']['next_steps']:
        html_content += f"<li>{step}</li>"
    
    html_content += """
            </ol>
        </div>
    </div>
</body>
</html>
    """
    
    html_path = f"{reports_dir}/llm_enhancement_report.html"
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"âœ… LLM ê³ ë„í™” ì™„ë£Œ!")
    print(f"ğŸ“„ JSON ë¦¬í¬íŠ¸: {json_path}")
    print(f"ğŸ“„ HTML ë¦¬í¬íŠ¸: {html_path}")
    print("=" * 60)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    summary = report['llm_enhancement_report']['accuracy_analysis']['test_summary']
    print(f"ğŸ¯ ë¶„ë¥˜ ì •í™•ë„: {summary['overall_accuracy']}%")
    print(f"âš¡ í‰ê·  ì‘ë‹µì‹œê°„: {summary['avg_response_time_ms']}ms")
    print(f"ğŸ’° ì˜ˆìƒ ì›”ê°„ ë¹„ìš©: ${summary['estimated_monthly_cost_usd']}")

if __name__ == "__main__":
    main()